\documentclass[14pt, a4paper]{book}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[ampersand]{easylist}

\date{\today}
\begin{document}
\tableofcontents
\title{Machine Learning Algorithms}
\author{Rangarajan R}
\maketitle
\chapter{Regression Algorithms}
\section{Ordinary Least Squares Regression (OLSR)}
\section{Linear Regression}
\section{Logistic Regression}
\section{Stepwise Regression} 
\section{Multivariate Adaptive Regression Splines (MARS)}
\section{Locally Estimated Scatterplot Smoothing (LOESS)}
\chapter{Instancebased Algorithms}
\section{k Nearest Neighbor (kNN)}
\section{Learning Vector Quantization (LVQ)}
\section{Self Organizing Map (SOM)}
\section{Locally Weighted Learning (LWL)}
\chapter{Regularization Algorithms}
\section{Ridge Regression}
\section{Least Absolute Shrinkage and Selection Operator (LASSO)}
\section{Elastic Net}
\section{Least Angle Regression (LARS)}
\chapter{Decision Tree Algorithms}
\section{Classification and Regression Tree (CART)}
\section{Iterative Dichotomiser 3 (ID3)}
\section{C45 and C50 (different versions of a powerful approach)}
\section{Chi squared Automatic Interaction Detection (CHAID)}
\section{Decision Stump}
\section{M5}
\section{Conditional Decision Trees}
\chapter{Bayesian Algorithms}
\section{Naive Bayes}
\section{Gaussian Naive Bayes}
\section{Multinomial Naive Bayes}
\section{Averaged One Dependence Estimators (AODE)}
\section{Bayesian Belief Network (BBN)}
\section{Bayesian Network (BN)}
\chapter{Clustering Algorithms}
\section{k Means}
\section{k Medians}
\section{Expectation Maximisation (EM)}
\section{Hierarchical Clustering}
\section{Association Rule Learning Algorithms}
\section{Apriori algorithm}
\section{Eclat algorithm}
\chapter{Artificial Neural Network Algorithms}
\section{Perceptron}
\section{Back Propagation}
\section{Hopfield Network}
\section{Radial Basis Function Network (RBFN)}
\chapter{Deep Learning Algorithms}
\section{Deep Boltzmann Machine (DBM)}
\section{Deep Belief Networks (DBN)}
\section{Convolutional Neural Network (CNN)}
\section{Stacked Auto Encoders}
\section{Recurrent Neural Network}
\chapter{Dimensionality Reduction Algorithms}
\section{Principal Component Analysis (PCA)}
\section{Principal Component Regression (PCR)}
\section{Partial Least Squares Regression (PLSR)}
\section{Sammon Mapping}
\section{Multidimensional Scaling (MDS)}
\section{Projection Pursuit}
\section{Linear Discriminant Analysis (LDA)}
\section{Mixture Discriminant Analysis (MDA)}
\section{Quadratic Discriminant Analysis (QDA)}
\section{Flexible Discriminant Analysis (FDA)}
\chapter{Ensemble Algorithms}
\section{Boosting}
\section{Bootstrapped Aggregation (Bagging)}
\section{AdaBoost}
\section{Stacked Generalization (blending)}
\section{Gradient Boosting Machines (GBM)}
\section{Gradient Boosted Regression Trees (GBRT)}
\section{Random Forest}
\section{K Nearest Neighbors(KNN) for Machine Learning}
 The model representation for KNN is the entire training dataset. \\
 KNN has no model other than storing the entire dataset, so there is
  no learning required.
\section{Predictions using KNN}
\subsection{Euclidean distance}
\subsection{Hamming Distance}
\subsection{Manhattan Distance}
\subsection{Minkowski Distance}
\subsection{Tanimoto distance}
\subsection{Jaccard distance}
\subsection{Mahalanobis distance}
\subsection{Cosine distance}
\section{different disciplines have different names for KNN}
\subsection{InstanceBased Learning}
\subsection{Lazy Learning}
\subsection{NonParametric}
\chapter{Natrual Language Processing}
\section{Basic feature extraction using text data}
We can use text data to extract a number of features even if we don\'t have sufficient knowledge of Natural Language Processing. So let\'s discuss some of them in this section.
\subsection{Number of words}
\subsection{Number of characters}
\subsection{Average word length}
\subsection{Number of stopwords}
\subsection{Number of special characters}
\subsection{Number of numerics}
\subsection{Number of uppercase words}
\section{Basic Text Preprocessing of text data}
\subsection{Lower casing}
\subsection{Punctuation removal}
\subsection{Stopwords removal}
\subsection{Frequent words removal}
\subsection{Rare words removal}
\subsection{Spelling correction}
\subsection{Tokenization}
\subsection{Stemming}
\subsection{Lemmatization}
\section{Advance Text Processing}
\subsection{Ngrams}
\subsection{Term Frequency}
\subsection{Inverse Document Frequency}
\subsection{Term Frequency Inverse Document Frequency (TF IDF)}
\subsection{Bag of Words}
\subsection{Sentiment Analysis}
\subsection{Word Embedding}
\chapter{Model Evaluation Error Metrics}
\section{Confusion Matrix}
\section{Gain and Lift Chart}
\section{Kolmogorov Smirnov Chart}
\section{AUC - ROC}
\section{Gini Coefficient}
\section{Concordant ¡V Discordant Ratio}
\section{Root Mean Squared Error}
\section{Cross Validation (Not a metric though)}

\end{document}
